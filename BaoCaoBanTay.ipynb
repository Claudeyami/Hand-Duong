{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410f0b1c",
   "metadata": {},
   "source": [
    "# B√°o C√°o Ph√¢n T√≠ch D·ª± √Ån G1 Dual Arm Grasping RL (v22)\n",
    "\n",
    "## üéØ M·ª•c Ti√™u B√°o C√°o\n",
    "\n",
    "B√°o c√°o n√†y ph√¢n t√≠ch chi ti·∫øt d·ª± √°n **G1 Dual Arm Grasping RL (v22)**, m·ªôt h·ªá th·ªëng hu·∫•n luy·ªán robot hai tay th·ª±c hi·ªán ƒë·ªông t√°c n·∫Øm v√† n√¢ng v·∫≠t th·ªÉ s·ª≠ d·ª•ng Reinforcement Learning trong MuJoCo simulator.\n",
    "\n",
    "**M·ª•c ti√™u ph√¢n t√≠ch:**\n",
    "- ‚úÖ Hi·ªÉu r√µ c·∫•u tr√∫c d·ª± √°n v√† c√°c th√†nh ph·∫ßn\n",
    "- ‚úÖ Ph√¢n t√≠ch chi ti·∫øt t·ª´ng file code v·ªõi code snippets ƒë·∫ßy ƒë·ªß\n",
    "- ‚úÖ Gi·∫£i th√≠ch nguy√™n l√Ω ho·∫°t ƒë·ªông v√† √Ω nghƒ©a c·ªßa t·ª´ng ph·∫ßn\n",
    "- ‚úÖ N·∫Øm b·∫Øt lu·ªìng ho·∫°t ƒë·ªông t·ªïng th·ªÉ c·ªßa h·ªá th·ªëng\n",
    "- ‚úÖ Hi·ªÉu c√°ch t√≠ch h·ª£p MuJoCo, Gymnasium, v√† Stable-Baselines3\n",
    "\n",
    "---\n",
    "\n",
    "## üìã M·ª•c L·ª•c\n",
    "\n",
    "1. [Gi·ªõi Thi·ªáu D·ª± √Ån](#1-gi·ªõi-thi·ªáu-d·ª±-√°n)\n",
    "2. [C·∫•u Tr√∫c Th∆∞ M·ª•c](#2-c·∫•u-tr√∫c-th∆∞-m·ª•c)\n",
    "3. [Ph√¢n T√≠ch Chi Ti·∫øt T·ª´ng File](#3-ph√¢n-t√≠ch-chi-ti·∫øt-t·ª´ng-file)\n",
    "   - 3.1. Environment (g1_dual_arm_env_v22.py)\n",
    "   - 3.2. Training Script (train_ppo_v22.py)\n",
    "   - 3.3. Evaluation Script (eval_policy_v22.py)\n",
    "   - 3.4. Demo Script (demo_pd_close_hand.py)\n",
    "   - 3.5. Utilities (common.py)\n",
    "4. [Nguy√™n L√Ω Ho·∫°t ƒê·ªông T·ªïng Th·ªÉ](#4-nguy√™n-l√Ω-ho·∫°t-ƒë·ªông-t·ªïng-th·ªÉ)\n",
    "5. [K·∫øt Lu·∫≠n v√† Nh·∫≠n X√©t](#5-k·∫øt-lu·∫≠n-v√†-nh·∫≠n-x√©t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe0d94",
   "metadata": {},
   "source": [
    "## 1. Gi·ªõi Thi·ªáu D·ª± √Ån\n",
    "\n",
    "### 1.1. T·ªïng Quan\n",
    "\n",
    "D·ª± √°n **G1 Dual Arm Grasping RL (v22)** l√† m·ªôt h·ªá th·ªëng hu·∫•n luy·ªán robot hai tay th·ª±c hi·ªán ƒë·ªông t√°c n·∫Øm v√† n√¢ng v·∫≠t th·ªÉ (cube) s·ª≠ d·ª•ng Reinforcement Learning (RL). D·ª± √°n s·ª≠ d·ª•ng:\n",
    "\n",
    "- **MuJoCo 3.x**: Physics simulator ti√™n ti·∫øn ƒë·ªÉ m√¥ ph·ªèng v·∫≠t l√Ω robot\n",
    "- **Gymnasium**: Framework chu·∫©n ƒë·ªÉ t·∫°o custom RL environments\n",
    "- **Stable-Baselines3**: Th∆∞ vi·ªán RL hi·ªán ƒë·∫°i h·ªó tr·ª£ PPO\n",
    "- **PyTorch**: Deep learning framework cho neural networks\n",
    "\n",
    "### 1.2. T√≠nh NƒÉng Ch√≠nh\n",
    "\n",
    "1. **Dual-arm robot**: ƒêi·ªÅu khi·ªÉn 22 actuators (14 arm joints + 10 finger synergy actuators + 2 fist macros)\n",
    "2. **Continuous grasping**: Synergy actuators ƒëi·ªÅu khi·ªÉn ph·ªëi h·ª£p 5 ng√≥n tay m·ªói tay ƒë·ªÉ n·∫Øm v·∫≠t th·ªÉ\n",
    "3. **Progress-based reward**: Reward d·ª±a tr√™n ti·∫øn tr√¨nh (reach ‚Üí contact ‚Üí lift)\n",
    "4. **Privileged observations**: Asymmetric observations cho critic (ground truth contact/poses)\n",
    "5. **Domain randomization**: Randomize physics parameters (friction, size, solver) ƒë·ªÉ tƒÉng robustness\n",
    "6. **Latency & sensor delay**: M√¥ ph·ªèng ƒë·ªô tr·ªÖ th·ª±c t·∫ø trong control v√† sensing\n",
    "7. **Safety mechanisms**: Termination khi wrist velocity qu√° cao ho·∫∑c state invalid\n",
    "8. **Curriculum learning**: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh ƒë·ªô kh√≥ d·ª±a tr√™n success rate\n",
    "\n",
    "### 1.3. C√¥ng Ngh·ªá S·ª≠ D·ª•ng\n",
    "\n",
    "| C√¥ng Ngh·ªá | Phi√™n B·∫£n | M·ª•c ƒê√≠ch |\n",
    "|-----------|-----------|----------|\n",
    "| **MuJoCo** | >=3.2.5 | Physics simulation engine |\n",
    "| **Gymnasium** | >=0.29.1 | RL environment interface |\n",
    "| **Stable-Baselines3** | >=2.3.2 | RL algorithms (PPO) |\n",
    "| **PyTorch** | >=2.3 | Deep learning framework |\n",
    "| **Python** | 3.10+ | Programming language |\n",
    "| **NumPy** | >=1.26 | Numerical computations |\n",
    "\n",
    "### 1.4. M·ª•c ƒê√≠ch ·ª®ng D·ª•ng\n",
    "\n",
    "D·ª± √°n n√†y c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ:\n",
    "- Hu·∫•n luy·ªán robot hai tay th·ª±c hi·ªán manipulation tasks ph·ª©c t·∫°p\n",
    "- Nghi√™n c·ª©u v·ªÅ continuous control trong RL v·ªõi privileged information\n",
    "- Ph√°t tri·ªÉn c√°c ·ª©ng d·ª•ng robotic manipulation v·ªõi contact-rich tasks\n",
    "- So s√°nh hi·ªáu qu·∫£ gi·ªØa c√°c reward shaping strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648588c7",
   "metadata": {},
   "source": [
    "## 2. C·∫•u Tr√∫c Th∆∞ M·ª•c\n",
    "\n",
    "### 2.1. M√¥ T·∫£ C·∫•u Tr√∫c Th∆∞ M·ª•c\n",
    "\n",
    "```\n",
    "g1_dual_arm_grasping_rl/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ envs/                          # Custom Gymnasium Environment\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # Package initialization\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ g1_dual_arm_env_v22.py    # ‚≠ê Main environment class (DualArmGraspEnvV22)\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ scripts/                       # Training v√† evaluation scripts\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train_ppo_v22.py          # ‚≠ê PPO training script v·ªõi VecNormalize\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ eval_policy_v22.py        # ‚≠ê Evaluation script cho trained models\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ demo_pd_close_hand.py     # Demo PD control (baseline)\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ utils/                         # Utility functions\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ common.py                 # Common argument parsers v√† helpers\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ models/                        # MuJoCo model files\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ g1_dual_arm.xml           # ‚≠ê Main MuJoCo XML model\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ meshes/                   # STL mesh files cho robot parts\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ runs/                          # Training outputs (models, logs)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ppo_dualarm_cuda_v22_prog_ft/  # PPO training outputs\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ppo_latest.zip        # Latest model checkpoint\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vecnorm_latest.pkl    # VecNormalize statistics\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ppo_ckpt_*.zip        # Periodic checkpoints\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ run_train.ps1                  # PowerShell script ƒë·ªÉ kh·ªüi ƒë·ªông training\n",
    "‚îú‚îÄ‚îÄ run_eval.ps1                   # PowerShell script ƒë·ªÉ ƒë√°nh gi√° model\n",
    "‚îú‚îÄ‚îÄ run_demo_pd.ps1                # PowerShell script ƒë·ªÉ ch·∫°y demo\n",
    "‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies\n",
    "‚îî‚îÄ‚îÄ README.md                      # Documentation\n",
    "```\n",
    "\n",
    "**C√°c file quan tr·ªçng nh·∫•t (ƒë∆∞·ª£c ƒë√°nh d·∫•u ‚≠ê):**\n",
    "1. **g1_dual_arm_env_v22.py**: Core environment class v·ªõi progress-based reward\n",
    "2. **train_ppo_v22.py**: Training script v·ªõi VecNormalize v√† checkpointing\n",
    "3. **eval_policy_v22.py**: Evaluation script v·ªõi VecNormalize loading\n",
    "4. **g1_dual_arm.xml**: MuJoCo physics model v·ªõi dual-arm robot v√† hands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# L·∫•y ƒë∆∞·ªùng d·∫´n g·ªëc c·ªßa d·ª± √°n\n",
    "project_root = Path(\".\")\n",
    "print(\"=\"*70)\n",
    "print(\"  üìÅ C·∫§U TR√öC TH∆Ø M·ª§C D·ª∞ √ÅN\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Li·ªát k√™ c·∫•u tr√∫c th∆∞ m·ª•c\n",
    "def print_tree(directory, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    \"\"\"In c·∫•u tr√∫c th∆∞ m·ª•c d·∫°ng tree\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    items = sorted(directory.iterdir())\n",
    "    dirs = [item for item in items if item.is_dir() and not item.name.startswith('.')]\n",
    "    files = [item for item in items if item.is_file()]\n",
    "    \n",
    "    # In th∆∞ m·ª•c tr∆∞·ªõc\n",
    "    for i, item in enumerate(dirs):\n",
    "        is_last_dir = (i == len(dirs) - 1) and len(files) == 0\n",
    "        print(f\"{prefix}{'‚îî‚îÄ‚îÄ' if is_last_dir else '‚îú‚îÄ‚îÄ'} {item.name}/\")\n",
    "        new_prefix = prefix + (\"    \" if is_last_dir else \"‚îÇ   \")\n",
    "        print_tree(item, new_prefix, max_depth, current_depth + 1)\n",
    "    \n",
    "    # In file sau\n",
    "    for i, item in enumerate(files):\n",
    "        is_last = i == len(files) - 1\n",
    "        print(f\"{prefix}{'‚îî‚îÄ‚îÄ' if is_last else '‚îú‚îÄ‚îÄ'} {item.name}\")\n",
    "\n",
    "print_tree(project_root)\n",
    "\n",
    "# Th·ªëng k√™ s·ªë l∆∞·ª£ng file\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  üìä TH·ªêNG K√ä D·ª∞ √ÅN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def count_files(directory, extensions=None):\n",
    "    \"\"\"ƒê·∫øm s·ªë file theo extension\"\"\"\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # B·ªè qua c√°c th∆∞ m·ª•c ·∫©n\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        for file in files:\n",
    "            if extensions is None or any(file.endswith(ext) for ext in extensions):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "py_files = count_files(project_root, ['.py'])\n",
    "xml_files = count_files(project_root, ['.xml'])\n",
    "stl_files = count_files(project_root, ['.stl'])\n",
    "\n",
    "print(f\"Python files (.py): {py_files}\")\n",
    "print(f\"XML model files (.xml): {xml_files}\")\n",
    "print(f\"STL mesh files (.stl): {stl_files}\")\n",
    "print(f\"Total Python files: {py_files}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65d3dd",
   "metadata": {},
   "source": [
    "## 3. Ph√¢n T√≠ch Chi Ti·∫øt T·ª´ng File\n",
    "\n",
    "### 3.1. File: `envs/g1_dual_arm_env_v22.py` - Core Environment\n",
    "\n",
    "ƒê√¢y l√† file **quan tr·ªçng nh·∫•t** c·ªßa d·ª± √°n, ƒë·ªãnh nghƒ©a custom Gymnasium environment cho robot G1 Dual Arm v·ªõi progress-based reward v√† privileged observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a22913",
   "metadata": {},
   "source": [
    "#### 3.1.1. Class DualArmGraspEnvV22 - Kh·ªüi T·∫°o\n",
    "\n",
    "**Code quan tr·ªçng:**\n",
    "\n",
    "```python\n",
    "class DualArmGraspEnvV22(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\",\"none\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xml_path: str = DEFAULT_XML,\n",
    "        render_mode: str | None = None,\n",
    "        max_steps: int = 400,\n",
    "        use_privileged: bool = True,\n",
    "        obs_noise_std: float = 0.004,\n",
    "        latency_steps: int = 2,\n",
    "        sensor_delay_steps: int = 1,\n",
    "        hold_steps: int = 120,\n",
    "        slip_torque_lim: float = 45.0,\n",
    "        max_wrist_angvel: float = 20.0,\n",
    "        strict_check: bool = True,\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load MuJoCo model\n",
    "        self.model = mj.MjModel.from_xml_path(self.xml_path)\n",
    "        self.data  = mj.MjData(self.model)\n",
    "        \n",
    "        # Map actuators (22 total: 14 arm + 10 finger + 2 fist)\n",
    "        self.actuator_names = ARM_ACTS + FINGER_SYNS + FIST_MACROS\n",
    "        self.aid = np.array([_safe_name2id(self.model, mj.mjtObj.mjOBJ_ACTUATOR, n)\n",
    "                             for n in self.actuator_names], dtype=int)\n",
    "        \n",
    "        # Action space: 22D (normalized [-1, 1])\n",
    "        n_act = len(self.actuator_names)\n",
    "        self.action_space = spaces.Box(low=-np.ones(n_act, np.float32),\n",
    "                                       high= np.ones(n_act, np.float32),\n",
    "                                       dtype=np.float32)\n",
    "        \n",
    "        # Observation space: dynamic based on sensors + joints + privileged\n",
    "        base_dim = 0\n",
    "        for sid in self.sids:\n",
    "            base_dim += int(self.model.sensor_dim[sid]) if sid >= 0 else 1\n",
    "        base_dim += len(self.jids)*2  # joint pos + vel\n",
    "        priv_dim = sum(dim for _, dim in self._priv_items) if self.use_privileged else 0\n",
    "        self.obs_dim = base_dim + priv_dim\n",
    "        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(self.obs_dim,), dtype=np.float32)\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch chi ti·∫øt:**\n",
    "\n",
    "1. **K·∫ø th·ª´a t·ª´ `gym.Env`**: \n",
    "   - Tu√¢n th·ªß Gymnasium API chu·∫©n\n",
    "   - Cho ph√©p t√≠ch h·ª£p v·ªõi Stable-Baselines3\n",
    "\n",
    "2. **MuJoCo Model Loading**:\n",
    "   - `MjModel.from_xml_path()`: Load XML model file v√†o MuJoCo\n",
    "   - `MjData()`: T·∫°o data structure ch·ª©a state (positions, velocities, controls)\n",
    "\n",
    "3. **Actuators (22 total)**:\n",
    "   - **ARM_ACTS (14)**: 7 joints m·ªói tay (shoulder pitch/roll/yaw, elbow, wrist roll/pitch/yaw)\n",
    "   - **FINGER_SYNS (10)**: 5 ng√≥n m·ªói tay (thumb, index, middle, ring, little)\n",
    "   - **FIST_MACROS (2)**: left_fist, right_fist (macro actuators)\n",
    "\n",
    "4. **Action Space (22D)**:\n",
    "   - Range: [-1.0, 1.0] (normalized)\n",
    "   - Mapped to actuator control ranges trong `_set_action()`\n",
    "\n",
    "5. **Observation Space (Dynamic)**:\n",
    "   - **Base sensors**: EE positions, quaternions, touch sensors, wrist velocities, palm forces/torques\n",
    "   - **Joint states**: 14 arm joints √ó 2 (pos + vel) = 28D\n",
    "   - **Privileged (optional)**: Cube qpos/qvel (13D), contact force sum (1D), plate normal z (1D)\n",
    "   - **Total**: ~50-70D t√πy sensors c√≥ trong XML\n",
    "\n",
    "6. **Key Parameters**:\n",
    "   - `use_privileged`: N·∫øu True, th√™m ground truth info cho critic (asymmetric obs)\n",
    "   - `obs_noise_std`: Gaussian noise cho observations (0.004)\n",
    "   - `latency_steps`: ƒê·ªô tr·ªÖ control (2 steps)\n",
    "   - `sensor_delay_steps`: ƒê·ªô tr·ªÖ sensor (1 step)\n",
    "   - `hold_steps`: S·ªë steps ph·∫£i gi·ªØ th√†nh c√¥ng ƒë·ªÉ ho√†n th√†nh task (120)\n",
    "   - `slip_torque_lim`: Ng∆∞·ª°ng torque ƒë·ªÉ ph√°t hi·ªán slip (45.0 N‚ãÖm)\n",
    "   - `max_wrist_angvel`: Ng∆∞·ª°ng angular velocity ƒë·ªÉ safety termination (20.0 rad/s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f1c05",
   "metadata": {},
   "source": [
    "#### 3.1.2. Observation Function (_obs_now)\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "def _obs_now(self):\n",
    "    arr = []\n",
    "    # Sensor data\n",
    "    for sid in self.sids:\n",
    "        v = self._sensor_vec(sid)\n",
    "        if v.size == 0: arr.append(0.0)\n",
    "        else:           arr.extend(v.tolist())\n",
    "    # Joint states (pos + vel)\n",
    "    q, v = self._arm_state()\n",
    "    arr.extend(q.tolist()); arr.extend(v.tolist())\n",
    "    # Privileged observations (if enabled)\n",
    "    if self.use_privileged: arr.extend(self._priv_obs().tolist())\n",
    "    return np.asarray(arr, dtype=np.float64)\n",
    "\n",
    "def _priv_obs(self):\n",
    "    out = []\n",
    "    # Cube state (qpos 7D + qvel 6D = 13D)\n",
    "    if self._jid_cube_free >= 0:\n",
    "        qadr = int(self.model.jnt_qposadr[self._jid_cube_free])\n",
    "        dadr = int(self.model.jnt_dofadr[self._jid_cube_free])\n",
    "        out.extend(self.data.qpos[qadr:qadr+7].tolist())\n",
    "        out.extend(self.data.qvel[dadr:dadr+6].tolist())\n",
    "    else:\n",
    "        out.extend([0.0]*13)\n",
    "    # Contact force sum (1D)\n",
    "    touch_sum = 0.0\n",
    "    for sid in self._touch_sids:\n",
    "        v = self._sensor_vec(sid); touch_sum += float(v[0]) if v.size else 0.0\n",
    "    out.append(touch_sum)\n",
    "    # Plate normal z (1D)\n",
    "    pz = 0.0\n",
    "    if self._sid_plate_top >= 0:\n",
    "        p = self._sensor_vec(self._sid_plate_top)\n",
    "        if p.size >= 3: pz = float(p[2])\n",
    "    out.append(pz)\n",
    "    return np.asarray(out, dtype=np.float64)\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch:**\n",
    "\n",
    "1. **Sensor data**: L·∫•y t·ª´ MuJoCo sensors (EE positions, quaternions, touch, velocities, forces)\n",
    "2. **Joint states**: 14 arm joints √ó 2 (position + velocity) = 28D\n",
    "3. **Privileged observations** (ch·ªâ cho critic, kh√¥ng cho actor):\n",
    "   - **Cube state**: Ground truth position/orientation v√† velocity (13D)\n",
    "   - **Contact force sum**: T·ªïng l·ª±c ti·∫øp x√∫c t·ª´ 5 fingertips (1D)\n",
    "   - **Plate normal z**: Z coordinate c·ªßa plate top (1D)\n",
    "4. **Noise augmentation**: Th√™m Gaussian noise v·ªõi std=0.004 v√†o observations\n",
    "5. **Sensor delay**: Observations ƒë∆∞·ª£c delay b·ªüi `sensor_delay_steps` (ring buffer)\n",
    "\n",
    "**√ù nghƒ©a**: Observation n√†y cung c·∫•p ƒë·∫ßy ƒë·ªß th√¥ng tin v·ªÅ tr·∫°ng th√°i robot v√† v·∫≠t th·ªÉ ƒë·ªÉ agent c√≥ th·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh ƒëi·ªÅu khi·ªÉn ph√π h·ª£p. Privileged info gi√∫p critic h·ªçc t·ªët h∆°n trong khi actor ch·ªâ d√πng public sensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969be3cf",
   "metadata": {},
   "source": [
    "#### 3.1.3. Action Application Function (_set_action)\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "def _set_action(self, action):\n",
    "    a = np.clip(np.asarray(action, np.float64), -1.0, 1.0)\n",
    "    u = self._smooth_and_limit(a)\n",
    "    self._jerk = float(np.linalg.norm(u - self._u_prev))\n",
    "    u_lat = self._apply_latency(u)\n",
    "    self._u_prev = u.copy()\n",
    "    for i, aid in enumerate(self.aid):\n",
    "        if aid < 0: continue\n",
    "        lo, hi = self.model.actuator_ctrlrange[aid]\n",
    "        self.data.ctrl[aid] = lo + 0.5*(u_lat[i]+1.0)*(hi-lo)\n",
    "\n",
    "def _smooth_and_limit(self, a):\n",
    "    n_arm = len(ARM_ACTS)\n",
    "    arm, fin = a[:n_arm].copy(), a[n_arm:].copy()\n",
    "    # Exponential smoothing\n",
    "    arm = self._alpha_arm*arm + (1-self._alpha_arm)*self._u_prev[:n_arm]\n",
    "    fin = self._alpha_finger*fin + (1-self._alpha_finger)*self._u_prev[n_arm:]\n",
    "    # Rate limiting\n",
    "    def limit(x, xprev, du): return xprev + np.clip(x-xprev, -du, du)\n",
    "    arm = limit(arm, self._u_prev[:n_arm], self._du_max_arm)\n",
    "    fin = limit(fin, self._u_prev[n_arm:], self._du_max_finger)\n",
    "    return np.concatenate([arm, fin])\n",
    "\n",
    "def _apply_latency(self, u):\n",
    "    self._ring_idx = (self._ring_idx + 1) % len(self._u_ring)\n",
    "    self._u_ring[self._ring_idx] = u.copy()\n",
    "    delayed_idx = (self._ring_idx - self.latency_steps) % len(self._u_ring)\n",
    "    return self._u_ring[delayed_idx]\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch chi ti·∫øt:**\n",
    "\n",
    "1. **Action preprocessing**:\n",
    "   - Clip v·ªÅ range [-1.0, 1.0]\n",
    "   - **Smoothing**: Exponential moving average (alpha_arm=0.4, alpha_finger=0.22)\n",
    "   - **Rate limiting**: Gi·ªõi h·∫°n thay ƒë·ªïi m·ªói step (du_max_arm=0.15, du_max_finger=0.08)\n",
    "   - **Jerk calculation**: T√≠nh ƒë·ªô \"gi·∫≠t\" c·ªßa control signal\n",
    "\n",
    "2. **Latency simulation**:\n",
    "   - Ring buffer ƒë·ªÉ delay action b·ªüi `latency_steps` (m·∫∑c ƒë·ªãnh 2 steps)\n",
    "   - M√¥ ph·ªèng ƒë·ªô tr·ªÖ th·ª±c t·∫ø trong control loop\n",
    "\n",
    "3. **Control mapping**:\n",
    "   - Map t·ª´ normalized action [-1, 1] ‚Üí actuator control range [lo, hi]\n",
    "   - Formula: `ctrl = lo + 0.5*(action+1.0)*(hi-lo)`\n",
    "\n",
    "4. **Grip reflex** (trong `step()`):\n",
    "   - T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh `right_fist` d·ª±a tr√™n touch v√† torque\n",
    "   - N·∫øu touch > 0.02 v√† torque < slip_limit ‚Üí tƒÉng grip\n",
    "   - N·∫øu torque > slip_limit*2.2 ‚Üí gi·∫£m grip (tr√°nh slip)\n",
    "\n",
    "**√ù nghƒ©a**: H√†m n√†y chuy·ªÉn ƒë·ªïi normalized actions t·ª´ agent th√†nh c√°c l·ªánh ƒëi·ªÅu khi·ªÉn th·ª±c t·∫ø cho robot v·ªõi smoothing, rate limiting, v√† latency ƒë·ªÉ m√¥ ph·ªèng ƒëi·ªÅu ki·ªán th·ª±c t·∫ø.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddd2f3d",
   "metadata": {},
   "source": [
    "#### 3.1.4. Reward Function (_compute_reward)\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "def _compute_reward(self, M):\n",
    "    # Progress-based rewards\n",
    "    r_reach_prog = 2.0 * (self._prev_dist - M[\"dist\"])\n",
    "    gate_contact = 1.0 if M[\"touch_sum\"] > 0.03 else 0.0\n",
    "    r_lift_prog = 6.0 * (M[\"dz_rel\"] - self._prev_dz_rel) * gate_contact\n",
    "    r_lift_prog = float(np.clip(r_lift_prog, -0.5, 1.0))\n",
    "    \n",
    "    # Contact reward\n",
    "    r_touch = 0.4*np.tanh(0.6*M[\"touch_sum\"]) + 0.4*(M[\"nf\"]>=2)*gate_contact\n",
    "    \n",
    "    # Penalties\n",
    "    a_ctrl = self.data.ctrl[self.aid[self.aid >= 0]]\n",
    "    r_pen = -0.0015*M[\"wrist_rate\"] \\\n",
    "            -0.0030*self._jerk \\\n",
    "            -0.0020*(float(np.linalg.norm(a_ctrl)) if a_ctrl.size>0 else 0.0) \\\n",
    "            -0.0025*max(0.0, M[\"tau_mag\"] - self.slip_torque_lim)\n",
    "    r_step = -0.002\n",
    "    \n",
    "    reward = r_reach_prog + r_lift_prog + r_touch + r_pen + r_step\n",
    "    \n",
    "    # Success condition\n",
    "    success = 1.0 if (M[\"dz_rel\"] > 0.04 and M[\"touch_sum\"] > 0.08) else 0.0\n",
    "    if success > 0.5: self._hold_counter += 1\n",
    "    else:             self._hold_counter = 0\n",
    "    if self._hold_counter >= self.hold_steps_required:\n",
    "        reward += 10.0  # Bonus for stable hold\n",
    "    \n",
    "    info = {\"success\":success, \"hold\": float(self._hold_counter >= self.hold_steps_required)}\n",
    "    info.update(M)\n",
    "    self._prev_dist = M[\"dist\"]; self._prev_dz_rel = M[\"dz_rel\"]\n",
    "    return reward, info\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch chi ti·∫øt:**\n",
    "\n",
    "1. **Progress-based rewards**:\n",
    "   - **r_reach_prog**: Reward khi gi·∫£m kho·∫£ng c√°ch ƒë·∫øn cube (2.0 √ó distance reduction)\n",
    "   - **r_lift_prog**: Reward khi n√¢ng cube l√™n (6.0 √ó height increase), ch·ªâ khi c√≥ contact (gated)\n",
    "   - **Gate contact**: Ch·ªâ cho reward lift khi `touch_sum > 0.03` (ƒë√£ ti·∫øp x√∫c)\n",
    "\n",
    "2. **Contact reward**:\n",
    "   - **r_touch**: Reward d·ª±a tr√™n t·ªïng l·ª±c ti·∫øp x√∫c (tanh ƒë·ªÉ saturate) + bonus n·∫øu ‚â•2 ng√≥n ch·∫°m\n",
    "\n",
    "3. **Penalties**:\n",
    "   - **Wrist rate**: Penalty cho angular velocity cao (tr√°nh quay nhanh)\n",
    "   - **Jerk**: Penalty cho control signal gi·∫≠t (smoothness)\n",
    "   - **Control magnitude**: Penalty cho effort (energy efficiency)\n",
    "   - **Slip torque**: Penalty khi torque v∆∞·ª£t ng∆∞·ª°ng slip (tr√°nh l√†m r∆°i)\n",
    "\n",
    "4. **Step penalty**: -0.002 m·ªói step (khuy·∫øn kh√≠ch ho√†n th√†nh nhanh)\n",
    "\n",
    "5. **Success condition**:\n",
    "   - `dz_rel > 0.04`: Cube ƒë∆∞·ª£c n√¢ng l√™n √≠t nh·∫•t 4cm\n",
    "   - `touch_sum > 0.08`: C√≥ ƒë·ªß l·ª±c ti·∫øp x√∫c\n",
    "   - **Hold bonus**: +10.0 reward n·∫øu gi·ªØ th√†nh c√¥ng trong `hold_steps` (120 steps)\n",
    "\n",
    "**√ù nghƒ©a**: Reward function n√†y khuy·∫øn kh√≠ch robot h·ªçc c√°ch ti·∫øp c·∫≠n, n·∫Øm, v√† n√¢ng v·∫≠t th·ªÉ m·ªôt c√°ch ·ªïn ƒë·ªãnh, v·ªõi penalties ƒë·ªÉ ƒë·∫£m b·∫£o smooth v√† safe motions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03d9a3",
   "metadata": {},
   "source": [
    "#### 3.1.5. Domain Randomization (_randomize_physics)\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "def _randomize_physics(self):\n",
    "    L = float(np.clip(self.curr_level, 0.0, 1.0))  # Curriculum level\n",
    "    gid = _safe_name2id(self.model, mj.mjtObj.mjOBJ_GEOM, \"grasp_cube_geom\")\n",
    "    if gid >= 0:\n",
    "        # Randomize cube size\n",
    "        sz = self.model.geom_size[gid].copy()\n",
    "        noise = self._rng.uniform(-0.003*L, 0.003*L, size=3)\n",
    "        self.model.geom_size[gid] = np.clip(sz + noise, 0.012, 0.030)\n",
    "        # Randomize cube friction\n",
    "        fr = self.model.geom_friction[gid].copy()\n",
    "        fr *= (1.0 + self._rng.uniform(-0.3*L, 0.3*L, size=3))\n",
    "        fr[0] = np.clip(fr[0], 0.5, 5.0); fr[1] = np.clip(fr[1], 0.005, 0.10); fr[2] = np.clip(fr[2], 0.002, 0.05)\n",
    "        self.model.geom_friction[gid] = fr\n",
    "    \n",
    "    # Randomize fingertip pad friction\n",
    "    names = [mj.mj_id2name(self.model, mj.mjtObj.mjOBJ_GEOM, i) for i in range(self.model.ngeom)]\n",
    "    for i, n in enumerate(names):\n",
    "        if n and n.endswith(\"_tip_pad\"):\n",
    "            fr = self.model.geom_friction[i].copy()\n",
    "            fr *= (1.0 + self._rng.uniform(-0.25*L, 0.25*L, size=3))\n",
    "            fr[0] = np.clip(fr[0], 3.0, 12.0); fr[1] = np.clip(fr[1], 0.02, 0.20); fr[2] = np.clip(fr[2], 0.002, 0.05)\n",
    "            self.model.geom_friction[i] = fr\n",
    "    \n",
    "    # Randomize solver parameters\n",
    "    def jitter_sol(i, sref_scale=0.15*L, simp_scale=0.005*L):\n",
    "        if hasattr(self.model, \"geom_solref\"):\n",
    "            sr = self.model.geom_solref[i].copy()\n",
    "            sr[0] *= 1.0 + self._rng.uniform(-sref_scale, sref_scale)\n",
    "            sr[1] *= 1.0 + self._rng.uniform(-sref_scale, sref_scale)\n",
    "            self.model.geom_solref[i] = sr\n",
    "        if hasattr(self.model, \"geom_solimp\"):\n",
    "            si = self.model.geom_solimp[i].copy()\n",
    "            si[:2] = np.clip(si[:2] + self._rng.uniform(-simp_scale, simp_scale, size=2), 0.90, 0.999)\n",
    "            self.model.geom_solimp[i] = si\n",
    "    \n",
    "    # Randomize actuator ranges\n",
    "    for aid in self.aid:\n",
    "        if aid < 0: continue\n",
    "        lo, hi = self.model.actuator_ctrlrange[aid]\n",
    "        span = (hi - lo) * (1.0 + self._rng.uniform(-0.10*L, 0.10*L))\n",
    "        self.model.actuator_ctrlrange[aid] = np.array([lo, lo+span], dtype=np.float64)\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch:**\n",
    "\n",
    "1. **Curriculum level (L)**: TƒÉng d·∫ßn t·ª´ 0.0 ‚Üí 1.0 d·ª±a tr√™n success rate\n",
    "   - N·∫øu `succ_ema > 0.35`: TƒÉng level (+0.002)\n",
    "   - N·∫øu `succ_ema <= 0.35`: Gi·∫£m level (-0.001)\n",
    "\n",
    "2. **Cube randomization**:\n",
    "   - **Size**: ¬±0.003*L (t·ªëi ƒëa ¬±3mm khi L=1.0)\n",
    "   - **Friction**: ¬±30%*L (t·ªëi ƒëa ¬±30% khi L=1.0)\n",
    "\n",
    "3. **Fingertip pad friction**: ¬±25%*L ƒë·ªÉ m√¥ ph·ªèng wear/conditions kh√°c nhau\n",
    "\n",
    "4. **Solver parameters**: Randomize `solref` v√† `solimp` ƒë·ªÉ m√¥ ph·ªèng contact dynamics kh√°c nhau\n",
    "\n",
    "5. **Actuator ranges**: ¬±10%*L ƒë·ªÉ m√¥ ph·ªèng actuator variations\n",
    "\n",
    "**√ù nghƒ©a**: Domain randomization gi√∫p policy robust h∆°n v·ªõi variations trong physics parameters, tƒÉng kh·∫£ nƒÉng transfer sang real robot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1200c0b",
   "metadata": {},
   "source": [
    "#### 3.1.6. Gymnasium API Methods (reset, step)\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "def reset(self, *, seed=None, options=None):\n",
    "    if seed is not None: self._rng = np.random.default_rng(seed)\n",
    "    if options and options.get(\"xml_path\") and options[\"xml_path\"] != self.xml_path:\n",
    "        self.xml_path = options[\"xml_path\"]\n",
    "        self.model = mj.MjModel.from_xml_path(self.xml_path); self.data = mj.MjData(self.model)\n",
    "        if self._viewer is not None:\n",
    "            try: self._viewer.close()\n",
    "            except: pass\n",
    "            self._viewer = None\n",
    "    self._reset_internal(randomize=True)\n",
    "    obs = self._apply_obs_noise(self._obs_now())\n",
    "    self._ring_idx = (self._ring_idx + 1) % len(self._obs_ring)\n",
    "    self._obs_ring[self._ring_idx] = obs\n",
    "    delayed_idx = (self._ring_idx - self.sensor_delay_steps) % len(self._obs_ring)\n",
    "    return self._obs_ring[delayed_idx].astype(np.float32), {}\n",
    "\n",
    "def step(self, action):\n",
    "    # Rate limit arm joint velocities\n",
    "    for j in self.jids:\n",
    "        if j >= 0:\n",
    "            dadr = int(self.model.jnt_dofadr[j])\n",
    "            self.data.qvel[dadr] = float(np.clip(self.data.qvel[dadr], -self._max_arm_qvel, self._max_arm_qvel))\n",
    "    \n",
    "    self._set_action(action)\n",
    "    \n",
    "    # Grip reflex (automatic fist adjustment)\n",
    "    # ... (code ƒë√£ tr√¨nh b√†y ·ªü tr√™n)\n",
    "    \n",
    "    mj.mj_step(self.model, self.data); self.step_count += 1\n",
    "    \n",
    "    # Get observation with delay\n",
    "    obs_now = self._apply_obs_noise(self._obs_now())\n",
    "    self._ring_idx = (self._ring_idx + 1) % len(self._obs_ring)\n",
    "    self._obs_ring[self._ring_idx] = obs_now\n",
    "    delayed_idx = (self._ring_idx - self.sensor_delay_steps) % len(self._obs_ring)\n",
    "    obs = self._obs_ring[delayed_idx].astype(np.float32)\n",
    "    \n",
    "    # Compute reward\n",
    "    M = self._compute_metrics()\n",
    "    reward, info = self._compute_reward(M)\n",
    "    \n",
    "    # Update curriculum\n",
    "    self._succ_ema = self._ema_beta*self._succ_ema + (1-self._ema_beta)*info[\"success\"]\n",
    "    if self._succ_ema > 0.35: self.curr_level = float(np.clip(self.curr_level + 0.002, 0.0, 1.0))\n",
    "    else:                      self.curr_level = float(np.clip(self.curr_level - 0.001, 0.0, 1.0))\n",
    "    if self.curr_level > 0.15: self.curr_phase = 1\n",
    "    if self.curr_level > 0.45: self.curr_phase = 2\n",
    "    \n",
    "    # Check termination\n",
    "    terminated = False; truncated = False\n",
    "    unsafe, reason = self._safety_termination(M)\n",
    "    if unsafe:\n",
    "        terminated = True; info[\"termination_reason\"] = reason\n",
    "    if info[\"hold\"] > 0.5:\n",
    "        terminated = True; info[\"termination_reason\"] = \"success_hold\"\n",
    "    if self.step_count >= self.max_steps: truncated = True\n",
    "    \n",
    "    if self.render_mode == \"human\" and self._viewer is not None:\n",
    "        self._viewer.sync()\n",
    "    \n",
    "    info[\"curr_level\"] = self.curr_level; info[\"succ_ema\"] = self._succ_ema\n",
    "    info[\"phase\"] = self.curr_phase; info[\"jerk\"] = self._jerk\n",
    "    return obs, reward, terminated, truncated, info\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch:**\n",
    "\n",
    "1. **reset()**:\n",
    "   - Reset environment v·ªÅ tr·∫°ng th√°i ban ƒë·∫ßu\n",
    "   - Randomize physics parameters\n",
    "   - Place cube tr√™n plate v·ªõi random offset\n",
    "   - Return delayed observation (sensor delay)\n",
    "\n",
    "2. **step()**:\n",
    "   - **Rate limit**: Clip arm joint velocities ƒë·ªÉ tr√°nh qu√° nhanh\n",
    "   - **Apply action**: G·ªçi `_set_action()` v·ªõi smoothing, rate limiting, latency\n",
    "   - **Grip reflex**: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh fist d·ª±a tr√™n touch/torque\n",
    "   - **Physics step**: `mj.mj_step()` - ti·∫øn simulation 1 b∆∞·ªõc\n",
    "   - **Get observation**: V·ªõi sensor delay (ring buffer)\n",
    "   - **Compute reward**: Progress-based v·ªõi gating\n",
    "   - **Update curriculum**: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh difficulty level\n",
    "   - **Check termination**:\n",
    "     - `terminated`: Safety violation ho·∫∑c success hold\n",
    "     - `truncated`: Max steps reached\n",
    "   - **Return**: (obs, rew, terminated, truncated, info)\n",
    "\n",
    "**√ù nghƒ©a**: Tu√¢n th·ªß Gymnasium API chu·∫©n, v·ªõi c√°c t√≠nh nƒÉng n√¢ng cao nh∆∞ curriculum learning, safety checks, v√† realistic delays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7b9b3",
   "metadata": {},
   "source": [
    "### 3.2. File: `scripts/train_ppo_v22.py` - PPO Training Script\n",
    "\n",
    "Ph√¢n t√≠ch script hu·∫•n luy·ªán v·ªõi thu·∫≠t to√°n PPO (Proximal Policy Optimization) v√† VecNormalize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d4233",
   "metadata": {},
   "source": [
    "#### 3.2.1. VecNormalize Setup\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "def make_vec(n_env, xml, use_priv, max_steps):\n",
    "    def _thunk(seed):\n",
    "        return lambda: make_env(xml_path=xml, render=False, use_privileged=use_priv, max_steps=max_steps, seed=seed)\n",
    "    env_fns = [_thunk(1234+i) for i in range(max(1, n_env))]\n",
    "    venv_raw = DummyVecEnv(env_fns) if n_env==1 else SubprocVecEnv(env_fns)\n",
    "    venv = VecNormalize(venv_raw, norm_obs=True, norm_reward=True, clip_obs=10.0, gamma=0.995)\n",
    "    return venv\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch:**\n",
    "\n",
    "1. **Vectorized environments**:\n",
    "   - `DummyVecEnv`: N·∫øu ch·ªâ 1 env (sequential)\n",
    "   - `SubprocVecEnv`: N·∫øu nhi·ªÅu env (parallel, m·ªói env trong process ri√™ng)\n",
    "\n",
    "2. **VecNormalize**:\n",
    "   - **norm_obs=True**: Normalize observations (running mean/std)\n",
    "   - **norm_reward=True**: Normalize rewards (running mean/std v·ªõi gamma=0.995)\n",
    "   - **clip_obs=10.0**: Clip observations v·ªÅ [-10, 10] ƒë·ªÉ tr√°nh outliers\n",
    "   - **gamma=0.995**: Discount factor cho reward normalization\n",
    "\n",
    "3. **L∆∞u VecNormalize stats**:\n",
    "   - File `vecnorm_latest.pkl` ch·ª©a running statistics\n",
    "   - **Quan tr·ªçng**: Ph·∫£i load c√πng stats khi evaluation!\n",
    "\n",
    "**√ù nghƒ©a**: VecNormalize gi√∫p stabilize training b·∫±ng c√°ch normalize observations v√† rewards, gi·∫£m variance v√† tƒÉng convergence speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d37abf",
   "metadata": {},
   "source": [
    "#### 3.2.2. PPO Model Configuration\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "policy_kwargs = dict(activation_fn=torch.nn.Tanh, net_arch=[dict(pi=[256,256], vf=[256,256])])\n",
    "model = PPO(\"MlpPolicy\", venv, n_steps=args.rollout, batch_size= args.rollout*max(1,args.n_env)//4,\n",
    "            learning_rate=2.5e-4, gamma=0.995, gae_lambda=0.95, clip_range=0.2, ent_coef=0.0,\n",
    "            verbose=1, device=args.device, policy_kwargs=policy_kwargs)\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch chi ti·∫øt c√°c hyperparameters:**\n",
    "\n",
    "1. **Policy**: `\"MlpPolicy\"` - Multi-layer perceptron (fully connected neural network)\n",
    "\n",
    "2. **Network architecture**:\n",
    "   - **Policy network (pi)**: [256, 256] hidden layers\n",
    "   - **Value network (vf)**: [256, 256] hidden layers\n",
    "   - **Activation**: Tanh (smooth, bounded)\n",
    "\n",
    "3. **Learning rate**: `2.5e-4` (0.00025)\n",
    "   - Standard cho PPO\n",
    "   - C√≥ th·ªÉ d√πng learning rate schedule n·∫øu c·∫ßn\n",
    "\n",
    "4. **n_steps**: `args.rollout` (m·∫∑c ƒë·ªãnh 1024)\n",
    "   - S·ªë steps m·ªói rollout tr∆∞·ªõc khi update policy\n",
    "   - L·ªõn h∆°n ‚Üí more stable nh∆∞ng slower\n",
    "\n",
    "5. **batch_size**: `rollout * n_env // 4`\n",
    "   - V√≠ d·ª•: 1024 * 4 // 4 = 1024\n",
    "   - Batch size cho training\n",
    "\n",
    "6. **gamma**: `0.995`\n",
    "   - Discount factor cho future rewards\n",
    "   - G·∫ßn 1.0 ‚Üí focus v√†o long-term rewards\n",
    "\n",
    "7. **gae_lambda**: `0.95`\n",
    "   - GAE (Generalized Advantage Estimation) lambda\n",
    "   - Bias-variance tradeoff cho advantage estimation\n",
    "\n",
    "8. **clip_range**: `0.2`\n",
    "   - PPO clipping range\n",
    "   - Gi·ªõi h·∫°n policy update ƒë·ªÉ tr√°nh qu√° l·ªõn\n",
    "\n",
    "9. **ent_coef**: `0.0`\n",
    "   - Entropy coefficient (exploration bonus)\n",
    "   - 0.0 = kh√¥ng c√≥ entropy bonus (c√≥ th·ªÉ d√πng curriculum thay th·∫ø)\n",
    "\n",
    "10. **Resume training**:\n",
    "    ```python\n",
    "    if args.resume:\n",
    "        model = PPO.load(args.resume, env=venv, device=args.device)\n",
    "        if args.reset_vf:\n",
    "            reinit_value_net(model)  # Reset value network\n",
    "    ```\n",
    "\n",
    "11. **Checkpoint callback**:\n",
    "    ```python\n",
    "    ckpt_cb = CheckpointCallback(\n",
    "        save_freq=max(1, args.save_every)//max(1,args.n_env),\n",
    "        save_path=logdir, name_prefix=\"ppo_ckpt\",\n",
    "        save_replay_buffer=False, save_vecnormalize=False\n",
    "    )\n",
    "    ```\n",
    "\n",
    "**√ù nghƒ©a**: Configuration n√†y ƒë∆∞·ª£c t·ªëi ∆∞u cho on-policy learning v·ªõi PPO, s·ª≠ d·ª•ng VecNormalize ƒë·ªÉ stabilize training v√† checkpointing ƒë·ªÉ resume training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a8fe4",
   "metadata": {},
   "source": [
    "### 3.3. File: `scripts/eval_policy_v22.py` - Evaluation Script\n",
    "\n",
    "Ph√¢n t√≠ch script ƒë√°nh gi√° trained models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547fb0d3",
   "metadata": {},
   "source": [
    "#### 3.3.1. Evaluation Logic\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    args = parse()\n",
    "    \n",
    "    def _mk():\n",
    "        return make_env(xml_path=args.xml,\n",
    "                        render=args.render,\n",
    "                        use_privileged=args.use_priv,\n",
    "                        max_steps=args.max_steps)\n",
    "    \n",
    "    # Load VecNormalize stats\n",
    "    venv_raw = DummyVecEnv([_mk])\n",
    "    venv = VecNormalize.load(args.venv_stats, venv_raw)\n",
    "    venv.training = False  # Kh√¥ng update stats khi eval\n",
    "    venv.norm_reward = False  # Kh√¥ng normalize reward khi eval\n",
    "    \n",
    "    model = PPO.load(args.ckpt)\n",
    "    \n",
    "    rets, succs, holds = [], [], []\n",
    "    for ep in range(args.episodes):\n",
    "        obs = venv.reset()\n",
    "        ep_ret, steps = 0.0, 0\n",
    "        \n",
    "        while True:\n",
    "            action, _ = model.predict(obs, deterministic=args.deterministic)\n",
    "            obs, rew, done, infos = venv.step(action)\n",
    "            ep_ret += float(rew[0])\n",
    "            steps  += 1\n",
    "            \n",
    "            info = infos[0] if isinstance(infos, (list, tuple)) else infos\n",
    "            if bool(done[0]):\n",
    "                s = int(info.get(\"success\", 0))\n",
    "                h = int(info.get(\"hold\", 0))\n",
    "                print(f\"[EP {ep:02d}] ret={ep_ret:.2f} | len={steps} | success={s} | hold={h} | curr_level={info.get('curr_level',0):.2f}\")\n",
    "                rets.append(ep_ret); succs.append(s); holds.append(h)\n",
    "                break\n",
    "    \n",
    "    print(\"\\n========== EVAL SUMMARY ==========\")\n",
    "    print(f\"Episodes           : {len(rets)}\")\n",
    "    print(f\"Mean reward        : {np.mean(rets):.2f} ¬± {np.std(rets):.2f}\")\n",
    "    print(f\"Success rate       : {100.0*sum(succs)/len(succs):.1f}%\")\n",
    "    print(f\"Hold (stable) rate : {100.0*sum(holds)/len(holds):.1f}%\")\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch:**\n",
    "\n",
    "1. **Load VecNormalize stats**:\n",
    "   - **Quan tr·ªçng**: Ph·∫£i load c√πng `vecnorm_latest.pkl` t·ª´ training run\n",
    "   - `venv.training = False`: Kh√¥ng update running stats khi eval\n",
    "   - `venv.norm_reward = False`: Kh√¥ng normalize reward (ƒë·ªÉ xem raw reward)\n",
    "\n",
    "2. **Load model**: `PPO.load(args.ckpt)` - Load checkpoint\n",
    "\n",
    "3. **Evaluation loop**:\n",
    "   - Reset environment m·ªói episode\n",
    "   - Predict action v·ªõi `deterministic=True/False`\n",
    "   - Accumulate reward v√† track metrics\n",
    "   - Collect: total reward, success rate, hold rate\n",
    "\n",
    "4. **Metrics**:\n",
    "   - **Mean reward**: Trung b√¨nh reward qua episodes\n",
    "   - **Success rate**: % episodes ƒë·∫°t success condition (dz_rel > 0.04, touch > 0.08)\n",
    "   - **Hold rate**: % episodes gi·ªØ th√†nh c√¥ng trong `hold_steps` (120 steps)\n",
    "\n",
    "**√ù nghƒ©a**: Script n√†y cung c·∫•p tool ƒë·ªÉ evaluate v√† so s√°nh trained models m·ªôt c√°ch chu·∫©n h√≥a, v·ªõi metrics r√µ r√†ng v·ªÅ performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600c978",
   "metadata": {},
   "source": [
    "### 3.4. File: `scripts/demo_pd_close_hand.py` - PD Control Demo\n",
    "\n",
    "File n√†y demo PD control ƒë·ªÉ ƒë√≥ng n·∫Øm tay (baseline, kh√¥ng d√πng RL).\n",
    "\n",
    "**L∆∞u √Ω**: File n√†y c√≥ import sai (c·∫ßn s·ª≠a `from envs.g1_dual_arm_env import DualArmGraspEnv` ‚Üí `from envs.g1_dual_arm_env_v22 import DualArmGraspEnvV22`).\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "import argparse, time, numpy as np\n",
    "from envs.g1_dual_arm_env_v22 import DualArmGraspEnvV22\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--xml\", type=str, default=\"models/g1_dual_arm.xml\")\n",
    "    ap.add_argument(\"--secs\", type=float, default=6.0)\n",
    "    ap.add_argument(\"--fist\", action=\"store_true\")\n",
    "    args = ap.parse_args()\n",
    "    \n",
    "    env = DualArmGraspEnvV22(xml_path=args.xml, render_mode=\"human\")\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    a = np.zeros(env.action_space.shape[0], dtype=np.float32)\n",
    "    if args.fist:\n",
    "        a[-2:] = 1.0  # left_fist, right_fist\n",
    "    \n",
    "    t_end = time.time() + float(args.secs)\n",
    "    while time.time() < t_end:\n",
    "        obs, rew, term, trunc, info = env.step(a)\n",
    "        if term or trunc:\n",
    "            env.reset()\n",
    "    env.close()\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch:**\n",
    "\n",
    "1. **M·ª•c ƒë√≠ch**: Demo ƒë∆°n gi·∫£n ƒë·ªÉ test MuJoCo viewer v√† XML model\n",
    "2. **Action**: Zero action (robot ·ªü rest pose), ho·∫∑c activate fist n·∫øu `--fist`\n",
    "3. **Loop**: Ch·∫°y trong `--secs` gi√¢y, reset n·∫øu episode k·∫øt th√∫c\n",
    "\n",
    "**√ù nghƒ©a**: Baseline ƒë·ªÉ so s√°nh v·ªõi RL policy, v√† ƒë·ªÉ test viewer/model tr∆∞·ªõc khi training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db74146",
   "metadata": {},
   "source": [
    "### 3.5. File: `utils/common.py` - Common Utilities\n",
    "\n",
    "File n√†y ch·ª©a c√°c helper functions cho argument parsing.\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "import os, argparse\n",
    "\n",
    "def add_xml_arg(p):\n",
    "    p.add_argument(\"--xml\", type=str, default=\"models/g1_dual_arm.xml\", help=\"Path to MuJoCo XML\")\n",
    "    return p\n",
    "\n",
    "def add_train_args(p):\n",
    "    p.add_argument(\"--total-steps\", type=int, default=400_000)\n",
    "    p.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--save-every\", type=int, default=100_000)\n",
    "    p.add_argument(\"--run-name\", type=str, default=None)\n",
    "    p.add_argument(\"--render\", action=\"store_true\")\n",
    "    return p\n",
    "\n",
    "def make_run_dir(prefix, run_name=None):\n",
    "    import time, pathlib\n",
    "    if run_name is None:\n",
    "        run_name = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    path = pathlib.Path(\"runs\")/f\"{prefix}_{run_name}\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    return str(path)\n",
    "```\n",
    "\n",
    "**Gi·∫£i th√≠ch:**\n",
    "\n",
    "1. **add_xml_arg()**: Helper ƒë·ªÉ th√™m XML path argument\n",
    "2. **add_train_args()**: Helper ƒë·ªÉ th√™m training arguments\n",
    "3. **make_run_dir()**: T·∫°o th∆∞ m·ª•c run v·ªõi timestamp n·∫øu kh√¥ng c√≥ t√™n\n",
    "\n",
    "**√ù nghƒ©a**: Utilities ƒë·ªÉ t√°i s·ª≠ d·ª•ng code v√† gi·ªØ consistency gi·ªØa c√°c scripts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a2a05",
   "metadata": {},
   "source": [
    "## 4. Nguy√™n L√Ω Ho·∫°t ƒê·ªông T·ªïng Th·ªÉ\n",
    "\n",
    "### 4.1. Lu·ªìng Ho·∫°t ƒê·ªông T·ªïng Quan\n",
    "\n",
    "```\n",
    "Start Training\n",
    "    ‚Üì\n",
    "Load XML Model (g1_dual_arm.xml)\n",
    "    ‚Üì\n",
    "Create VecEnv v·ªõi VecNormalize\n",
    "    ‚Üì\n",
    "Initialize PPO Model\n",
    "    ‚Üì\n",
    "For each training iteration:\n",
    "    ‚îú‚îÄ Collect Rollouts (n_steps √ó n_env)\n",
    "    ‚îú‚îÄ Compute Advantages (GAE)\n",
    "    ‚îú‚îÄ Update Policy (multiple epochs)\n",
    "    ‚îú‚îÄ Update Value Function\n",
    "    ‚îú‚îÄ Save Checkpoint (periodic)\n",
    "    ‚îî‚îÄ Update VecNormalize Stats\n",
    "    ‚Üì\n",
    "Save Final Model + VecNormalize Stats\n",
    "    ‚Üì\n",
    "Evaluation v·ªõi eval_policy_v22.py\n",
    "```\n",
    "\n",
    "### 4.2. Chi Ti·∫øt Lu·ªìng Training\n",
    "\n",
    "#### 4.2.1. PPO Training Flow\n",
    "\n",
    "```\n",
    "1. Initialize PPO model v·ªõi MlpPolicy\n",
    "   - Policy network: [256, 256] hidden layers\n",
    "   - Value network: [256, 256] hidden layers\n",
    "   - Activation: Tanh\n",
    "\n",
    "2. Create VecEnv v·ªõi VecNormalize\n",
    "   - Parallel environments (SubprocVecEnv n·∫øu n_env > 1)\n",
    "   - Normalize observations v√† rewards\n",
    "   - Running statistics ƒë∆∞·ª£c update m·ªói step\n",
    "\n",
    "3. For each iteration:\n",
    "   a. Collect 1024 steps √ó n_env (rollout)\n",
    "   b. Compute advantages v·ªõi GAE (lambda=0.95)\n",
    "   c. Update policy v√† value function:\n",
    "      - Batch size: rollout * n_env // 4\n",
    "      - Clip range: 0.2\n",
    "      - Learning rate: 2.5e-4\n",
    "   d. Update VecNormalize statistics\n",
    "   e. Save checkpoint m·ªói save_every steps\n",
    "\n",
    "4. Save final model v√† VecNormalize stats\n",
    "   - ppo_latest.zip\n",
    "   - vecnorm_latest.pkl\n",
    "```\n",
    "\n",
    "### 4.3. Environment Interaction Flow\n",
    "\n",
    "```\n",
    "Agent ‚Üí Action (22D normalized [-1, 1])\n",
    "    ‚Üì\n",
    "_set_action():\n",
    "  - Clip action v·ªÅ [-1, 1]\n",
    "  - Smoothing (exponential moving average)\n",
    "  - Rate limiting (du_max_arm=0.15, du_max_finger=0.08)\n",
    "  - Apply latency (ring buffer, delay 2 steps)\n",
    "  - Map to actuator control ranges\n",
    "    ‚Üì\n",
    "MuJoCo Physics Step (mj_step):\n",
    "  - Forward kinematics\n",
    "  - Dynamics simulation\n",
    "  - Collision detection\n",
    "  - Update state (qpos, qvel, site_xpos)\n",
    "    ‚Üì\n",
    "_obs_now():\n",
    "  - Get sensor data (EE pos, quat, touch, velocities, forces)\n",
    "  - Get joint states (qpos, qvel)\n",
    "  - Get privileged obs (if enabled): cube state, contact sum, plate z\n",
    "  - Add observation noise (std=0.004)\n",
    "  - Apply sensor delay (ring buffer, delay 1 step)\n",
    "    ‚Üì\n",
    "_compute_reward():\n",
    "  - Progress rewards: reach_prog, lift_prog (gated by contact)\n",
    "  - Contact reward: touch_sum, n_fingers\n",
    "  - Penalties: wrist_rate, jerk, control_mag, slip_torque\n",
    "  - Step penalty: -0.002\n",
    "  - Hold bonus: +10.0 n·∫øu gi·ªØ th√†nh c√¥ng 120 steps\n",
    "    ‚Üì\n",
    "Update curriculum level:\n",
    "  - N·∫øu succ_ema > 0.35: tƒÉng level (+0.002)\n",
    "  - N·∫øu succ_ema <= 0.35: gi·∫£m level (-0.001)\n",
    "  - Update domain randomization v·ªõi level m·ªõi\n",
    "    ‚Üì\n",
    "Check termination:\n",
    "  - Safety: wrist_angvel > 20.0, NaN, invalid state\n",
    "  - Success: hold >= 120 steps\n",
    "  - Truncation: step_count >= max_steps\n",
    "    ‚Üì\n",
    "Return (obs, rew, terminated, truncated, info)\n",
    "```\n",
    "\n",
    "### 4.4. Reward Shaping Logic\n",
    "\n",
    "**Reward Components:**\n",
    "\n",
    "1. **r_reach_prog** (max ~2.0):\n",
    "   - Formula: `2.0 * (prev_dist - current_dist)`\n",
    "   - Reward khi gi·∫£m kho·∫£ng c√°ch ƒë·∫øn cube\n",
    "\n",
    "2. **r_lift_prog** (max ~1.0, gated):\n",
    "   - Formula: `6.0 * (dz_rel - prev_dz_rel) * gate_contact`\n",
    "   - Gate: `1.0 if touch_sum > 0.03 else 0.0`\n",
    "   - Clipped: `[-0.5, 1.0]`\n",
    "   - Reward khi n√¢ng cube l√™n, ch·ªâ khi ƒë√£ c√≥ contact\n",
    "\n",
    "3. **r_touch** (max ~0.8):\n",
    "   - Formula: `0.4*tanh(0.6*touch_sum) + 0.4*(n_fingers>=2)*gate_contact`\n",
    "   - Reward cho contact quality\n",
    "\n",
    "4. **r_pen** (negative):\n",
    "   - `-0.0015*wrist_rate`: Penalty cho angular velocity cao\n",
    "   - `-0.0030*jerk`: Penalty cho control signal gi·∫≠t\n",
    "   - `-0.0020*control_mag`: Penalty cho effort\n",
    "   - `-0.0025*max(0, tau_mag - slip_torque_lim)`: Penalty cho slip risk\n",
    "\n",
    "5. **r_step**: `-0.002` (khuy·∫øn kh√≠ch ho√†n th√†nh nhanh)\n",
    "\n",
    "6. **Hold bonus**: `+10.0` n·∫øu gi·ªØ th√†nh c√¥ng 120 steps\n",
    "\n",
    "**Total Reward**: `r_reach_prog + r_lift_prog + r_touch + r_pen + r_step + hold_bonus`\n",
    "\n",
    "**Success Condition**: \n",
    "- `dz_rel > 0.04` (n√¢ng l√™n 4cm)\n",
    "- `touch_sum > 0.08` (ƒë·ªß l·ª±c ti·∫øp x√∫c)\n",
    "\n",
    "### 4.5. MuJoCo Integration\n",
    "\n",
    "**MuJoCo Model Structure:**\n",
    "- **XML file**: `models/g1_dual_arm.xml`\n",
    "  - Defines robot structure (bodies, joints, actuators)\n",
    "  - Defines physics properties (masses, inertias, friction)\n",
    "  - Defines control limits (forcerange, ctrlrange)\n",
    "  - Defines sensors (EE pos, quat, touch, velocities, forces)\n",
    "  \n",
    "- **Actuators (22 total)**:\n",
    "  - **Arm motors (14)**: Torque control v·ªõi forcerange trong XML\n",
    "  - **Finger synergy (10)**: Tendon-based control v·ªõi ctrlrange trong XML\n",
    "  - **Fist macros (2)**: Macro actuators cho left_fist, right_fist\n",
    "  \n",
    "- **Joints**:\n",
    "  - **Arm joints (14)**: Shoulder, elbow, wrist joints\n",
    "  - **Hand joints**: Finger MCP, PIP, DIP joints (kh√¥ng ƒëi·ªÅu khi·ªÉn tr·ª±c ti·∫øp)\n",
    "  - **Cube free joint**: 7 DoF (3 pos + 4 quat) cho cube\n",
    "  \n",
    "- **Sensors**:\n",
    "  - **EE positions**: `right_ee_pos`, `left_ee_pos`\n",
    "  - **EE quaternions**: `right_ee_quat`, `left_ee_quat`\n",
    "  - **Touch sensors**: `right_thumb_touch`, `right_index_touch`, ...\n",
    "  - **Wrist velocities**: `right_wrist_vang`, `right_wrist_vlin`\n",
    "  - **Palm forces/torques**: `right_palm_force`, `right_palm_torque`\n",
    "  - **Cube/plate positions**: `cube_top_pos`, `plate_top_pos`\n",
    "\n",
    "**Simulation Loop:**\n",
    "```python\n",
    "# 1. Set controls\n",
    "for i, aid in enumerate(self.aid):\n",
    "    if aid < 0: continue\n",
    "    lo, hi = self.model.actuator_ctrlrange[aid]\n",
    "    self.data.ctrl[aid] = lo + 0.5*(action[i]+1.0)*(hi-lo)\n",
    "\n",
    "# 2. Step physics\n",
    "mj.mj_step(self.model, self.data)\n",
    "\n",
    "# 3. Read state\n",
    "qpos = self.data.qpos  # Joint positions\n",
    "qvel = self.data.qvel  # Joint velocities\n",
    "sensordata = self.data.sensordata  # Sensor readings\n",
    "```\n",
    "\n",
    "### 4.6. Data Flow Diagram\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    Reinforcement Learning Loop               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Agent (PPO Policy Network)                                 ‚îÇ\n",
    "‚îÇ  Input: Observation (50-70D, normalized)                     ‚îÇ\n",
    "‚îÇ  Output: Action (22D, normalized [-1, 1])                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  VecNormalize                                                ‚îÇ\n",
    "‚îÇ  - Normalize observations (running mean/std)                 ‚îÇ\n",
    "‚îÇ  - Normalize rewards (running mean/std)                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Environment (DualArmGraspEnvV22)                           ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ  ‚îÇ  _set_action():                                      ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Smoothing (EMA)                                   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Rate limiting                                     ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Latency (ring buffer, 2 steps)                    ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Map to actuator ranges                           ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îÇ                              ‚îÇ                               ‚îÇ\n",
    "‚îÇ                              ‚ñº                               ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ  ‚îÇ  MuJoCo Physics (mj_step):                           ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Forward kinematics                               ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Dynamics simulation                              ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Collision detection                             ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Update state (qpos, qvel, sensordata)         ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îÇ                              ‚îÇ                               ‚îÇ\n",
    "‚îÇ                              ‚ñº                               ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ  ‚îÇ  _obs_now():                                         ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Get sensor data (EE, touch, velocities, forces)   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Get joint states (qpos, qvel)                   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Get privileged obs (cube, contact, plate)        ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Add observation noise (std=0.004)                ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Apply sensor delay (ring buffer, 1 step)          ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îÇ                              ‚îÇ                               ‚îÇ\n",
    "‚îÇ                              ‚ñº                               ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ  ‚îÇ  _compute_reward():                                 ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Progress rewards (reach, lift)                   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Contact reward                                   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Penalties (wrist_rate, jerk, effort, slip)      ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Hold bonus (+10.0)                               ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îÇ                              ‚îÇ                               ‚îÇ\n",
    "‚îÇ                              ‚ñº                               ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ  ‚îÇ  Curriculum Learning:                                ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Update level d·ª±a tr√™n success rate               ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  - Update domain randomization                      ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "                    Return (obs, rew, done, info)\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Training Algorithm (PPO)                                    ‚îÇ\n",
    "‚îÇ  - Compute advantages (GAE)                                 ‚îÇ\n",
    "‚îÇ  - Update policy network (clipped)                          ‚îÇ\n",
    "‚îÇ  - Update value network                                     ‚îÇ\n",
    "‚îÇ  - Update VecNormalize statistics                           ‚îÇ\n",
    "‚îÇ  - Save checkpoints                                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733f2c3",
   "metadata": {},
   "source": [
    "## 5. K·∫øt Lu·∫≠n v√† Nh·∫≠n X√©t\n",
    "\n",
    "### 5.1. T·ªïng K·∫øt D·ª± √Ån\n",
    "\n",
    "D·ª± √°n **G1 Dual Arm Grasping RL (v22)** l√† m·ªôt h·ªá th·ªëng ho√†n ch·ªânh ƒë·ªÉ hu·∫•n luy·ªán robot hai tay th·ª±c hi·ªán ƒë·ªông t√°c n·∫Øm v√† n√¢ng v·∫≠t th·ªÉ s·ª≠ d·ª•ng Reinforcement Learning. D·ª± √°n n√†y th·ªÉ hi·ªán:\n",
    "\n",
    "1. **T√≠ch h·ª£p t·ªët c√°c c√¥ng ngh·ªá**:\n",
    "   - MuJoCo 3.x physics simulator\n",
    "   - Gymnasium RL framework\n",
    "   - Stable-Baselines3 RL algorithms (PPO)\n",
    "   - PyTorch deep learning framework\n",
    "\n",
    "2. **Thi·∫øt k·∫ø environment chuy√™n nghi·ªáp**:\n",
    "   - Custom Gymnasium environment v·ªõi API chu·∫©n\n",
    "   - Progress-based reward shaping v·ªõi gating\n",
    "   - Privileged observations cho critic (asymmetric)\n",
    "   - Domain randomization v·ªõi curriculum learning\n",
    "   - Realistic delays (latency, sensor delay)\n",
    "\n",
    "3. **Training infrastructure**:\n",
    "   - VecNormalize ƒë·ªÉ stabilize training\n",
    "   - Parallel environments (SubprocVecEnv)\n",
    "   - Checkpointing v√† resume training\n",
    "   - Evaluation script chu·∫©n h√≥a\n",
    "\n",
    "4. **Safety v√† robustness**:\n",
    "   - Safety termination (wrist velocity, NaN checks)\n",
    "   - Rate limiting v√† smoothing cho controls\n",
    "   - Grip reflex ƒë·ªÉ tr√°nh slip\n",
    "   - Domain randomization ƒë·ªÉ tƒÉng robustness\n",
    "\n",
    "### 5.2. ƒêi·ªÉm M·∫°nh\n",
    "\n",
    "1. **Code organization t·ªët**:\n",
    "   - C·∫•u tr√∫c th∆∞ m·ª•c r√µ r√†ng\n",
    "   - Separation of concerns (env, training, evaluation)\n",
    "   - Reusable components (common.py)\n",
    "\n",
    "2. **Advanced features**:\n",
    "   - Progress-based reward v·ªõi gating\n",
    "   - Privileged observations (asymmetric actor-critic)\n",
    "   - Curriculum learning t·ª± ƒë·ªông\n",
    "   - Domain randomization\n",
    "   - Realistic delays (latency, sensor delay)\n",
    "\n",
    "3. **Robustness**:\n",
    "   - Noise augmentation (observation noise)\n",
    "   - Safety mechanisms\n",
    "   - Error handling\n",
    "   - VecNormalize ƒë·ªÉ stabilize training\n",
    "\n",
    "4. **Documentation**:\n",
    "   - README chi ti·∫øt v·ªõi quickstart\n",
    "   - Code comments r√µ r√†ng\n",
    "   - PowerShell scripts ƒë·ªÉ d·ªÖ ch·∫°y\n",
    "\n",
    "### 5.3. ƒêi·ªÉm C·∫ßn C·∫£i Thi·ªán\n",
    "\n",
    "1. **Demo script**:\n",
    "   - Import sai (c·∫ßn s·ª≠a `g1_dual_arm_env` ‚Üí `g1_dual_arm_env_v22`)\n",
    "   - C√≥ th·ªÉ th√™m nhi·ªÅu demo scenarios\n",
    "\n",
    "2. **Evaluation**:\n",
    "   - C√≥ th·ªÉ th√™m visualization (trajectory plots, contact forces)\n",
    "   - C√≥ th·ªÉ th√™m metrics chi ti·∫øt h∆°n (grasp quality, stability)\n",
    "\n",
    "3. **Training**:\n",
    "   - C√≥ th·ªÉ th√™m learning rate schedule\n",
    "   - C√≥ th·ªÉ th√™m early stopping\n",
    "   - C√≥ th·ªÉ th√™m TensorBoard logging chi ti·∫øt h∆°n\n",
    "\n",
    "4. **Environment**:\n",
    "   - C√≥ th·ªÉ th√™m nhi·ªÅu objects kh√°c nhau (kh√¥ng ch·ªâ cube)\n",
    "   - C√≥ th·ªÉ th√™m obstacles\n",
    "   - C√≥ th·ªÉ th√™m bimanual coordination tasks ph·ª©c t·∫°p h∆°n\n",
    "\n",
    "### 5.4. ·ª®ng D·ª•ng Th·ª±c T·∫ø\n",
    "\n",
    "D·ª± √°n n√†y c√≥ th·ªÉ ƒë∆∞·ª£c m·ªü r·ªông cho:\n",
    "\n",
    "1. **Robotic manipulation**:\n",
    "   - Object grasping v√† manipulation v·ªõi nhi·ªÅu objects\n",
    "   - Hand-eye coordination\n",
    "   - Bimanual coordination tasks ph·ª©c t·∫°p\n",
    "   - Transfer learning sang real robot\n",
    "\n",
    "2. **Research**:\n",
    "   - Continuous control trong RL v·ªõi privileged information\n",
    "   - Curriculum learning strategies\n",
    "   - Domain randomization techniques\n",
    "   - Asymmetric actor-critic architectures\n",
    "\n",
    "3. **Industrial applications**:\n",
    "   - Pick and place tasks\n",
    "   - Assembly tasks\n",
    "   - Quality inspection v·ªõi manipulation\n",
    "\n",
    "### 5.5. K·∫øt Lu·∫≠n\n",
    "\n",
    "D·ª± √°n **G1 Dual Arm Grasping RL (v22)** l√† m·ªôt implementation t·ªët c·ªßa RL cho robotic manipulation tasks v·ªõi contact-rich dynamics. Code ƒë∆∞·ª£c t·ªï ch·ª©c t·ªët, d·ªÖ hi·ªÉu v√† m·ªü r·ªông. V·ªõi reward shaping ph√π h·ª£p, privileged observations, curriculum learning, v√† domain randomization, d·ª± √°n n√†y c√≥ th·ªÉ serve nh∆∞ m·ªôt baseline ho·∫∑c starting point cho c√°c nghi√™n c·ª©u v√† ·ª©ng d·ª•ng th·ª±c t·∫ø.\n",
    "\n",
    "**Khuy·∫øn ngh·ªã**:\n",
    "- S·ª≠ d·ª•ng d·ª± √°n n√†y nh∆∞ reference implementation cho manipulation tasks\n",
    "- M·ªü r·ªông cho c√°c tasks ph·ª©c t·∫°p h∆°n (multiple objects, obstacles)\n",
    "- Th·ª≠ nghi·ªám v·ªõi c√°c thu·∫≠t to√°n RL kh√°c (SAC, TD3, ...)\n",
    "- T√≠ch h·ª£p v·ªõi real robot hardware\n",
    "- Th√™m visualization v√† analysis tools\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
